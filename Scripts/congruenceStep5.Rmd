---
title: "Towards a congruence measure creation"
author: "Biagio Palese"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning=FALSE)

```
#Abstract

Topic models are increasingly used by IS reaserchers. However, one of the major issue with this methodology is to evaluate the models to establish trustworthiness in their results (Eickhoff and Neuss 2017). In this report we propose a new method to assess topic model results quality. Human evaluation are considered the best method to assess topics interpretability, researchers and companies should make sure that topic models have the ability to extract topic that represent well documents' content (Chang et al. 2009). In this paper we report the steps followed to develop a measure of congurence between documents that can inform companies in taking relevant business decision. In this scenario the assessment of the topic per document quality is required to have results that are reliable. For this reason we propose a new method for topic models evaluation that is scalable and able to lead to a model selection based on actual accuracy. The procedure is explained in the context of hotels' online reviews. However, the method is generic and can be applied to other industries. The congruence measure implications are also discussed.  

#Introduction

The objective of this report is to provide an update towards the creation of a congruence measure. The measure should enable the research to understand how much the content of one document is aligned to the one of another document. In the context of this study we want to measure how much a managerial response content is close to the topics discussed in the review that triggered the response. So the level of the analysis is not the aggregate use of response from companies but rather each individual review response pair. Given that the measure is based on the textual content available in each review response pair we decide to use topic models to create the measure. Topic models enable to extract the hidden thematic structure from text. In acedemia, one of the main issue in the use of this methodology is how to prove that this methodology is rigourous, that the model selected is able to reliably extract the topics and that they are actually interpretable. In literature, different statistical measures (e.g, perplexity, hold out likelihood) exist to assess the predictive capability of the algorithm. They are generally used to determine the optimal number of topics to extract. However, statistical optimization of the algorithm doesnâ€™t always translate to high interpretability for human. Chang et. al 2009, recognize how important is for companies to make sure that topic models results are useful for them and that topic extracted are evaluated based on their ability to represents reality. They propose two measures to assess topic interpretability using human evaluation: word intrusion and topic intrusion. Both these measure rely on a survey based experiment where humans were asked to identify which word (topic in the second measure) is not related to the other that compose the topic (document). Interpretability of the topic while underestimated is the final goal of this reasearch. In fact, our measure is based on the ability of the algorithm to identify correctly the topics in documents. So, we can then compare the content of two documents. In this way companies are informed about what their customers are discusssing in their reviews, and they know if their response to them is customized or not. For this reason, we propose a new human evaluation of topics extracted.\newline 
We asked 5 humans to manually label 500 documents according to the topics they are able to clearly identify. We limited the number of topics to 7 hospitality related elements: service, location, room, food, hotel amenities, value, and greetings and salutations. These topics resulted from the combination of the five elements of the lodging service experience with an element common to managerial responses (greetings and salutations) and an element rated by reviewers on the major travel websites (hotel amenities). By doing so we created a golden standard set of document and their topics. We will then run multiple models and select the one that is closer to the human categorization. In this way we performed a human validation on the model selection based on interpretability of the topic per document extracted. We decide to call this approach as topic inclusion. In fact, the topic model parametrization is selected based on the ability of the algoritmh to identify the topic when it is there and when it is not. At the contrary of other evaluation metrics we are not evaluating statistically the predictive ability of the model on future datasets because here the focus is on the assessment of the topic per document quality. Once we are sure that the topics extract rapresent the topic discussed in the document text we can proceed in the calculation of the measure. On topics identified by the optimal model we will use cosine similarity and hellinger distance to calculate the congruence between each pair. Below are described the steps followed to create the golden set standard.




## Golden standard (Procedure)
The creation of the golden standard includes the following steps:

####Step 1: Introduction to the validation procedure
Four undergraduate students (2 male and 2 female) were selected to perform the labeling. The leader researcher organized a meeting with all the labelers and provided a validation procedure (to be included in an appendix) that includes the definition of each topic in the context of the study and some examples of them from real review/response available in the dataset.\ 
After the labelers read all the procedure, the researcher answered questions and provided clarification to them. After this stage, we requested them to label the first 10 pairs of the random subset. All labelers had the same random subset of review/response. Each labeler could assign as many topics as identified in the text of each pair. In case, the labelers believed that none of the 7 topics were discussed in the text he/she should label them as unclassifiable. The leading researcher also performed the validation but all labelers worked independently. When all the categorization was completed, each review was discussed individually to make sure that the topics were clear in labelers' mind and more explanation and/or clarification was provided by the researcher in case of disagreement. However, the original categorization was not modified even if after the discussion the labelers understood their mistakes. Before moving to the next step the research calculated the agreement between labelers including himself. We establish two measure of agreement, full and partial. Full agreement is achieved when in each document all the topics are indentified by all 5 labelers. While partial agreement consist in keeping in each documents only those topics that are identified by all reviewers. Full agreement was 70%, partial was 100%. So we proceeded to the next step.

####Step 2: Independent labeling (from 10 to 100 pairs) stage
In this step we moved from 10 to 100 pairs of review/response, individually rated by each labeler. In this subset, the labelers found sentences that were borderline between topic definition. The subjective judment in those cases made the full agreement drop to 29% and the partial to 86%. This drop indicate the need to call a new meeting to clarify the borderline situations. 

####Step 2b: Definition clarification stage

During the meeting we discuss those reviews and responses in which a topic was identified by only 1 or 2 labelers. After, the discussion, some of the topic definition have been modified to make clearer the boundaries between topics. Also all the reviews and responses with topic identified by only 1 or 2 labelers were reviewed together. In this, phase labelers were able to modify the previous topics assigned in case they agreed with what discussed or in case the modified definition of the topic changed their original judgment. After the meeting the full agreement jumped to 66% and the partial to 99%. For this reason we decided to move to the next step.

####Step 3: Independent labeling (from 100 to 300 pairs) stage
In this step we moved from 100 to 300 pairs of review/responses, individually rated by each labeler. At the end of this step we registered a new drop in terms of full agreeement (34.33%). That said, as we discussed above the full agreement requires all the reviewers to identify exactly the same topic review in each review. Given that this constraints at the end of step 3 looked too difficult to achieve even for humans. In fact, the content of the review is subject to labelers' interpretion in some cases even if we provided clear definitions for each topic. Moreover, the language used in a review is far from academic or professionals standard. we decided to move to the next step because at the contrary the partial agreement didn't register a significant drop (98.3%). We believe in fact, that partial agreement is sufficient to create the golden standard set.

####Step 4: Independent labeling (from 300 to 500 pairs) stage
In this step we moved from 300 to 500 pairs of review/responses, individually rated by each labeler. We label all the documents randomly selected from our dataset.  The results of this step indicate a further reduction of the full agreement (28.4%) and of the partial agreement (98.2%). We decided to have one final meeting in which we discussed the situation that were closer to a full agreement. So, when a topic was identified by 4 out of 5 rater and when a topic was identified by only 1 rater. The results of 5th and final steps are reported below and are used for the selection of the model used for the creation of the congruence measure.


```{r, results='hide', warning=FALSE}
###Data
##GoldenStandard set

library(knitr)
library(tidyverse)
load("/Users/Biagio/Desktop/biagio/Projects/ta/parameterTop/topicsCombined.rda")
goldenSet <- df_topicsCombined[, c(1:5)]
goldenSet$group <- gsub("([[:alpha:]]+)","",goldenSet$reviewId)#create a variable that identifies the review/response pairs
goldenSet2 <- goldenSet %>% arrange(group)# I order the dataset by the group, the objective is to select random group and run the validation of individual sentences as well as reviews of complete pairs of review response
goldenSet2$index <- c(1:length(goldenSet2$id))# I create a new column that uniquely identify each sentence
x <- goldenSet2 %>% distinct(group)#create a vector with unique group number, I will randomly select group for my validation
set.seed(0100)
s <- sample(x$group,250)#random sample of 500 groups on which we will create our golden set
goldenSetlabel <- filter(goldenSet2,group %in%s)# final dataset subject to manual labeling
#save(goldenSetLabel, file = "goldenSetValidation.rda")
```

```{r}
#Review level validation-golden standard set
goldenSetRevRes <- goldenSetlabel %>%  select(fullText,group,reviewId) %>% distinct(group,fullText,reviewId)
#write.csv(goldenSetRevRes, file = "./goldenSetRevRes.csv") #added in excel topic labels and notes for the those working on the validation--> file goldenSetRevRes.xlsx
```

####Step 5: Consolidation stage
The last step of the validation is discussed extensively in the next sections.

####Review/Response pairs dataset
```{r}

load("validationStepsdf/validationStep5.rda")


reviewTopicStep5 <- tidyValidationStep5 %>% group_by(reviewID,topicIdentified,topicValidator) %>% summarise() %>% na.omit()
reviewlabelsStep5 <- reviewTopicStep5 %>% group_by(reviewID, topicIdentified) %>% count()
sameLabelStep5 <- reviewTopicStep5 %>% group_by(reviewID, topicIdentified) %>% count()%>% 
  filter(n>4)#real golden set--> those reviews/response in which I have full agreement
sameLabelStep5$n <- sub("5|6|7|8", "5", sameLabelStep5$n)# If more than 5 reviewers identified a topic means that some reviewer inserted the same topic more than once so we consider 5,6,7,8 equivalent
differentLabelStep5 <-  reviewTopicStep5 %>% group_by(reviewID, topicIdentified) %>% count()%>% 
  filter(n<5)
``` 
I now take in consideration only those review that have all the topics in common or those that have no one topic that is label by all the raters inside the review
```{r, results='hide'}
agreementStep5 <- setdiff(unique(sameLabelStep5$reviewID), unique(differentLabelStep5$reviewID))#find those reviewID in which there is full agreement
disagreementStep5<- setdiff( unique(differentLabelStep5$reviewID),unique(sameLabelStep5$reviewID))#find those id in which there is full disagreement

```
We also investigate how many documents have full agreement between labelers; meaning  all validators identified the exact same topics in each review or response. There are `r length(agreementStep5)` documents with complete agremeement between labelers. Moreover, we found how many document have complete disagreement. Meaning, that in these documents there is not a single topic identified by all validators. There are only `r length(disagreementStep5)` documents with complete disagreement over `r length(unique(tidyValidationStep5$reviewID))` documents categorized.
So if the dataset of documents with full agreement is the following:
```{r }

reviewTopicfullAgreementStep5 <- sameLabelStep5 %>% group_by(reviewID) %>% spread(topicIdentified, topicIdentified) %>% filter(reviewID %in% agreementStep5)
colnames(reviewlabelsStep5) <- c("reviewId", "topicIdentified", "n")
reviewTopic_all_Step5 <- reviewlabelsStep5 %>% group_by(reviewId) %>% spread(topicIdentified, topicIdentified) 
prova <- right_join(goldenSetRevRes,reviewTopic_all_Step5)
#reviewTopic_all_Step5 %>% (reviewId %in% goldenSetRevRes)
glimpse(reviewTopicfullAgreementStep5)
glimpse(reviewTopic_all_Step5)
```

\pagebreak

Finally, we decided to create a dataset in which for each document we retain only the topics labeled by all validators (partial agreement). Also this dataset is used to assess how much the topic extracted by each model are close to humans labeling. Themodel selection is based on how close is the identification of the topics in each document compared to humans.Please see the dataset below:
```{r}
reviewTopicpartialAgreementStep5 <- sameLabelStep5 %>% group_by(reviewID) %>% spread(topicIdentified, topicIdentified)
glimpse(reviewTopicpartialAgreementStep5)
```

We want now to see how many of topics has been identified by all the validators in all the review/response and how many are identified in total by the 5 labelers. 
```{r}

table1 <- sameLabelStep5 %>% group_by(topicIdentified) %>%  summarise(Number= n()) %>% 
    mutate(Percentage = round((Number/ sum(Number))*100, 2))
colnames(table1) <- c("Topics", "Count", "Percentage")
kable(table1, caption = "Topics extracted by all five labelers")

```
```{r}
step5 <-Filter(function(x) !(all(x=="")), validationStep5)[,- c( 37:38)]# removing all the columns that contains all NA and rd columns that contains a comments that was suppose to be in the notes and bp notes
#colnames(test) <- c("id", "fullText", "rater1", "rater1", "rater1", "rater1", "rater1", "rater1",  "rater2", "rater2", "rater2", "rater2", "rater2", "rater2", "rater3", "rater3", "rater3", "rater3", "rater4", "rater4", "rater4", "rater4", "rater4", "rater4", "rater5", "rater5", "rater5", "rater5", "rater5", "rater5")


#rename all the validator name
tidystep5 <- gather(step5, topicValidator, topicIdentified, Topic1bp:Topic6nicka)
tidystep5$topicValidator <- sub("Topic1bp|Topic6bp|Topic2bp|Topic3bp|Topic4bp|Topic5bp|Topic7bp","rater1", tidystep5$topicValidator)
tidystep5$topicValidator <- sub("Topic1rd|Topic6rd|Topic2rd|Topic3rd|Topic4rd|Topic5rd|Topic7rd","rater2", tidystep5$topicValidator)
tidystep5$topicValidator <- sub("Topic1ew|Topic6ew|Topic2ew|Topic3ew|Topic4ew|Topic5ew","rater3", tidystep5$topicValidator)
tidystep5$topicValidator <- sub("Topic1am|Topic6am|Topic2am|Topic3am|Topic4am|Topic5am","rater4", tidystep5$topicValidator)
tidystep5$topicValidator <- sub("Topic1nicka|Topic6nicka|Topic2nicka|Topic3nicka|Topic4nicka|Topic5nicka","rater5", tidystep5$topicValidator)
tidystep5$topicIdentified <- tidystep5$topicIdentified %>% tolower()
#table(tidystep5$topicIdentified)
tidystep5$topicIdentified <- sub("greeting and salutations|greetings and salutation|reetings and salutations|greetings and salutations|greetings and saluations","greetings and salutations", tidystep5$topicIdentified) 
tidystep5$topicIdentified <- gsub("hotel aminities", "hotel amenities",tidystep5$topicIdentified)
tidystep5$topicIdentified <- gsub("unclassified","unclassifiable",tidystep5$topicIdentified)
tidystep5$topicIdentified <- gsub("serivce","service",tidystep5$topicIdentified)
tidystep5$topicIdentified <- gsub("locations","location",tidystep5$topicIdentified)
tidystep5$topicIdentified <- gsub("foom","food",tidystep5$topicIdentified)

tidystep5$topicIdentified[13955] <- NA
table2 <- tidystep5 %>% group_by(topicIdentified) %>% na.omit() %>%  summarise(Number= n()) %>% 
    mutate(Percentage = round((Number/ sum(Number))*100, 2))
colnames(table2) <- c("Topics", "Count", "Percentage")
kable(table2, caption = "Total number of topics identified")

step5Topic <- tidystep5 %>% group_by(reviewID,topicIdentified,topicValidator)  %>% summarize () %>% na.omit()
rater1 <- filter(step5Topic, topicValidator == "rater1")
rater2 <- filter(step5Topic, topicValidator == "rater2")
rater3 <- filter(step5Topic, topicValidator == "rater3")
rater4 <- filter(step5Topic, topicValidator == "rater4")
rater5 <- filter(step5Topic, topicValidator == "rater5")
raterTopics <-step5Topic %>% group_by(reviewID, topicIdentified) %>% spread(topicValidator, topicValidator)
#rater1Topic <-rater1 %>% group_by(reviewID) %>% spread(topicIdentified, topicIdentified)see each rater topics identified

```
#LEGGI FINO A QUI!
#inter rater reliability
```{r}
# rater1_topics <- rater1 %>% group_by(reviewID) %>% 
#      summarise(topics_rater1  = as.factor(paste(topicIdentified, collapse =",")))
# rater2_topics <- rater2 %>% group_by(reviewID) %>% 
#      summarise(topics_rater2  = as.factor(paste(topicIdentified, collapse =",")))
# rater3_topics <- rater3 %>% group_by(reviewID) %>% 
#      summarise(topics_rater3  = as.factor(paste(topicIdentified, collapse =",")))
# rater4_topics <- rater4 %>% group_by(reviewID) %>% 
#      summarise(topics_rater4  = as.factor(paste(topicIdentified, collapse =",")))
# rater5_topics <- rater5 %>% group_by(reviewID) %>% 
#      summarise(topics_rater5  = as.factor(paste(topicIdentified, collapse =",")))
# raters <- left_join(rater1_topics,rater2_topics)
# raters <- left_join(raters, rater3_topics)
# raters <- left_join(raters, rater4_topics)
# raters <- left_join(raters, rater5_topics)
# 
# rater1_food <- rater1 %>% group_by(reviewID) %>% 
#      filter(topicIdentified =="food")
# rater2_food <- rater2 %>% group_by(reviewID) %>% 
#    filter(topicIdentified =="food")
# rater3_food <- rater3 %>% group_by(reviewID) %>% 
#      filter(topicIdentified =="food")
# rater4_food <- rater4 %>% group_by(reviewID) %>% 
#     filter(topicIdentified =="food")
# rater5_food <- rater5 %>% group_by(reviewID) %>% 
#     filter(topicIdentified =="food")
# ratersF <- left_join(rater1_food,rater2_food)
# ratersF <- left_join(ratersF, rater3_food)
# ratersF <- left_join(raters, rater4_food)
# ratersF <- left_join(raters, rater5_food)
# 
# 
# 
# save(raters, rater1_topics, rater2_topics, rater3_topics, rater4_topics, rater5_topics, file = "raters_fleiss.rda")
load("raters_fleiss.rda")

ratersN <- raters
ratersN <- ratersN[,-1] %>%
  mutate_all(as.character)
#ratersN <- ratersN %>%
  #mutate_all(gsub("service", 1, ratersN$topics_rater1))
#ratersN[ratersN=="service"]<-1
library(irr)
kappam.fleiss(raters[,-1])

#load("full_topic/full_topicsCombined_eight_bi_3.rda")#change model here
#load("/Users/Biagio/Desktop/biagio/Projects/Validation/full_topic/full_topicsCombined_eight_sent_4.rda")
load("/Users/Biagio/Desktop/biagio/Projects/Validation/latestSeed/full_topicsCombined_latestSeed_3.rda")
tm <- dfmodel
tm$reviewTopicsTM <- gsub("1","service",tm$reviewTopicsTM)
tm$reviewTopicsTM <- gsub("2","value",tm$reviewTopicsTM)
tm$reviewTopicsTM <- gsub("3","location",tm$reviewTopicsTM)
tm$reviewTopicsTM <- gsub("4","room",tm$reviewTopicsTM)
tm$reviewTopicsTM <- gsub("5","food",tm$reviewTopicsTM)
tm$reviewTopicsTM <- gsub("6","greetings and salutations",tm$reviewTopicsTM)
tm$reviewTopicsTM <- gsub("7","hotel amenities",tm$reviewTopicsTM)
tm$topics_tm <- as.factor(tm$reviewTopicsTM)
tm <- tm[,c(1,9)]
colnames(tm) <- c("reviewID", "topics_tm")
raters_all6 <- left_join(raters, tm)
kappam.fleiss(raters_all6[,-1])
raters_4htm_1_no5 <- left_join(rater1_topics,rater2_topics)
raters_4htm_1_no5 <- left_join(raters_4htm_1_no5, rater3_topics)
raters_4htm_1_no5 <- left_join(raters_4htm_1_no5, rater4_topics)
raters_4htm_1_no5 <- left_join(raters_4htm_1_no5, tm)
kappam.fleiss(raters_4htm_1_no5[,-1])
raters_4htm_1_no4 <- left_join(rater1_topics,rater2_topics)
raters_4htm_1_no4 <- left_join(raters_4htm_1_no4, rater3_topics)
raters_4htm_1_no4 <- left_join(raters_4htm_1_no4, rater5_topics)
raters_4htm_1_no4 <- left_join(raters_4htm_1_no4, tm)
kappam.fleiss(raters_4htm_1_no4[,-1])
raters_4htm_1_no3 <- left_join(rater1_topics,rater2_topics)
raters_4htm_1_no3 <- left_join(raters_4htm_1_no3, rater4_topics)
raters_4htm_1_no3 <- left_join(raters_4htm_1_no3, rater5_topics)
raters_4htm_1_no3 <- left_join(raters_4htm_1_no3, tm)
kappam.fleiss(raters_4htm_1_no3[,-1])
raters_4htm_1_no2 <- left_join(rater1_topics,rater5_topics)
raters_4htm_1_no2 <- left_join(raters_4htm_1_no2, rater3_topics)
raters_4htm_1_no2 <- left_join(raters_4htm_1_no2, rater4_topics)
raters_4htm_1_no2 <- left_join(raters_4htm_1_no2, tm)
kappam.fleiss(raters_4htm_1_no2[,-1])
raters_4htm_1_no1 <- left_join(rater2_topics,rater5_topics)
raters_4htm_1_no1 <- left_join(raters_4htm_1_no1, rater3_topics)
raters_4htm_1_no1 <- left_join(raters_4htm_1_no1, rater4_topics)
raters_4htm_1_no1 <- left_join(raters_4htm_1_no1, tm)
kappam.fleiss(raters_4htm_1_no1[,-1])
# topicM_threshold <- topicM[,-9]
# topicM_threshold[topicM_threshold<0.05] <- NA#change multiple threshold to see how it changes
# colnames(topicM_threshold) <- c("reviewID","service","value","location","room","food", "greetings and salutations", "hotel amenities")
# topicM_threshold <- topicM_threshold[,c(1,6,7,8,4,5,2,3)]
# tm <-topicM_threshold %>%  filter(reviewID %in%raters$reviewID) 
# tm$service[is.na(tm$service)==FALSE] <-  "service"
# tm$value[is.na(tm$value)==FALSE] <-  "value"
# tm$location[is.na(tm$location)==FALSE] <-  "location"
# tm$room[is.na(tm$room)==FALSE] <-  "room"
# tm$food[is.na(tm$food)==FALSE] <-  "food"
# tm$`greetings and salutations`[is.na(tm$`greetings and salutations`)==FALSE] <-  "greetings and salutations"
# tm$`hotel amenities`[is.na(tm$`hotel amenities`)==FALSE] <-  "hotel amenities"
# 
# tm$topics_tm <- as.factor(apply(tm[,-1], 1, function(x) toString(na.omit(x))))
# tm <- tm[,c(1,9)]



# #tm %>% group_by(reviewID) %>% 
#      #summarise(topics_tm  = as.factor(apaste(x, collapse =",")))
# 
# topicM_threshold$names <- apply(topicM_threshold[-1], 1, FUN = function(x) paste(names(head(sort(x[x!=0], decreasing = TRUE), 7)), collapse=","))
# tm <-topicM_threshold %>%  filter(reviewID %in%raters$reviewID) 
# 
# 
# lapply(tm$names, function(x) sort(x))


```





\pagebreak

#### Topics by labelers

From the chart it is clear that some topics are almost consistently identified by each reviewer (greetings and slautations, value, service, room, location), while other have more variability (food and hotel amenities)
```{r}

topicbyValidator <- step5Topic %>% group_by (topicValidator, topicIdentified) %>% count()#topic identifyied by each validator


ggplot(data= filter(topicbyValidator, topicIdentified != "NA"), mapping = aes(x=topicIdentified, y=n, fill= topicIdentified))+
  geom_bar(position="dodge",stat="identity")+
  facet_grid(topicValidator ~.)+ 
   coord_flip()+
    labs(
    title = "Figure 1: Topic identified by each rater",
     x = "Topics by rater",
     y = "Number of topics identified"
   )+
  geom_text(aes(label=n), position=position_dodge(width=0.5), vjust=-0.1)


```

Percent agreement by topics is reported below. The proportion confirm the comments made above. More specifically we have four topics with more than 90% agreement(greetings and slautations, service, room, location). While value agreement is above 88%, food agreement is around 81% and hotel amenities is the only topic with agreement below 80%.  

```{r}
agreementbytopic <- step5Topic %>% group_by(topicIdentified, reviewID) %>% summarise(n= n()) %>%   mutate(Percentage = round((n/ 5)*100, 2))
proportionAgreement<- agreementbytopic %>% group_by(topicIdentified) %>% summarise(topic_proportion = round(mean(Percentage),2))
ggplot(data= proportionAgreement, mapping = aes(x=topicIdentified, y=topic_proportion, fill= topicIdentified))+
  geom_bar(position="dodge",stat="identity")+
    labs(
      title = "Figure 2: Per topic agreement between raters",
     x = "Topics",
     y = "Percent agreement by topic"
   )+
  geom_text(aes(label=topic_proportion), position=position_dodge(width=0.5), vjust=-0.1)



```



We than divided the  topics by the number of labelers that identified them. 5 represents a full agreement in terms of topics (this doesn't translate in full agreement in terms of document because each document is composed by multiple topics). In fact, those are the topics that all the labelers identified. 1 represents no agreement, only one labeler identify that topic per document. 
```{r}
topicbyValidatorperId <- step5Topic %>% group_by (topicValidator, topicIdentified, reviewID) %>% count()#topic identifided in the different id
#step5Topic %>% group_by (topicIdentified) %>% count()#number topic identified
reviewsR <- tidystep5 %>% group_by(reviewID,topicIdentified,topicValidator)  %>% summarize () %>% count() %>% na.omit()
topicbyid<- step5Topic %>% group_by (reviewID) %>% count()#topic identified per review, if the n is not a multiple of 5 means there is not agreement, doesn't apply to 6 means someb
#table(reviewsR$n)
### to be reviewed review
disagreement <- reviewsR %>% filter(n<5)
#unique(disagreement$reviewID)

table3 <- reviewsR %>% group_by(n)  %>%  summarise(Number= n()) %>% 
    mutate(Percentage = round((Number/ sum(Number))*100, 2))
colnames(table3) <- c("Number of labelers", "Number of topics identified", "Percentage of topics identified")
kable(table3, caption = "Number of topics by number of labelers")


```

\pagebreak

## Weakly-supervised topic models using Gibbs sampling

In this report we report want to provide a representation of how th method for model selection works. So, we decide to compare results from 3 modles with different parametrization and assess which one of them is more aligned with the human labeling ( golden standard set). However, our decision is purely pragmatic and there is no limitation on the number of models you can run and compare. The models used in this validation have the following parametrization: \newline 
    - Model 1: k = 7, seedwords = 15% of the total number of documents, alpha = 1,delta = 0.01, burnin = 500, iter = 1000, thin = 100. \newline 
    - Model 2:k = 7, seedwords = 15% of the total number of documents, alpha = 0.01,delta = 0.01, burnin = 500, iter = 1000, thin = 100. \newline 
    - Model 3: k = 7, seedwords = 15% of the total number of documents, alpha = 7,delta = 0.1, burnin = 500, iter = 1000, thin = 100. \newline 

Results of the topics model are reported below, providing the top ten words for each topic.

```{r, warning=FALSE}
library(tm)
library(topicmodels)
stopwordsXPO6 <-  c("a", "about", "above", "above", "across", "after", "afterwards", "again", "against", "all", "almost", "alone", "along", "already", "also","although","always","am","among", "amongst", "amoungst", "amount",  "an", "and", "another", "any","anyhow","anyone","anything","anyway", "anywhere", "are", "around", "as",  "at", "back","be","became", "because","become","becomes", "becoming", "been", "before", "beforehand", "behind", "being", "below", "beside", "besides", "between", "beyond", "bill", "both", "bottom","but", "by", "call", "can", "cannot", "cant", "co", "con", "could", "couldnt", "cry", "de", "describe", "detail", "do", "done", "down", "due", "during", "each", "eg", "eight", "either", "eleven","else", "elsewhere", "empty", "enough", "etc", "even", "ever", "every", "everyone", "everything", "everywhere", "except", "few", "fifteen", "fify", "fill", "find", "fire", "first", "five", "for", "former", "formerly", "forty", "found", "four", "from", "front", "full", "further", "get", "give", "go", "had", "has", "hasnt", "have", "he", "hence", "her", "here", "hereafter", "hereby", "herein", "hereupon", "hers", "herself", "him", "himself", "his", "how", "however", "hundred", "ie", "if", "in", "inc", "indeed", "interest", "into", "is", "it", "its", "itself", "keep", "last", "latter", "latterly", "least", "less", "ltd", "made", "many", "may", "me", "meanwhile", "might", "mill", "mine", "more", "moreover", "most", "mostly", "move", "much", "must", "my", "myself", "name", "namely", "neither", "never", "nevertheless", "next", "nine", "no", "nobody", "none", "noone", "nor", "not", "nothing", "now", "nowhere", "of", "off", "often", "on", "once", "one", "only", "onto", "or", "other", "others", "otherwise", "our", "ours", "ourselves", "out", "over", "own","part", "per", "perhaps", "please", "put", "rather", "re", "same", "see", "seem", "seemed", "seeming", "seems", "serious", "several", "she", "should", "show", "side", "since", "sincere", "six", "sixty", "so", "some", "somehow", "someone", "something", "sometime", "sometimes", "somewhere", "still", "such", "system", "take", "ten", "than", "that", "the", "their", "them", "themselves", "then", "thence", "there", "thereafter", "thereby", "therefore", "therein", "thereupon", "these", "they", "thickv", "thin", "third", "this", "those", "though", "three", "through", "throughout", "thru", "thus", "to", "together", "too", "top", "toward", "towards", "twelve", "twenty", "two", "un", "under", "until", "up", "upon", "us", "very", "via", "was", "we", "well", "were", "what", "whatever", "when", "whence", "whenever", "where", "whereafter", "whereas", "whereby", "wherein", "whereupon", "wherever", "whether", "which", "while", "whither", "who", "whoever", "whole", "whom", "whose", "why", "will", "with", "within", "without", "would", "yet", "you", "your", "yours", "yourself", "yourselves", "the")
#salutation <- c("dear", "best", "regards", "regard", "thank", "thanks","sincerely","please")#we want them they part of a topic
FILE <- "corpus_dtm_combinedRevRes.rda"
if (FILE %in% list.files()) load(FILE) else {
  myReader <- readTabular(mapping=list(content="fullText", id="reviewId"))
  corpus_data <- VCorpus(DataframeSource(goldenSetRevRes), readerControl=list(reader=myReader))
  corpus_data <- tm_map(corpus_data,content_transformer(removePunctuation),preserve_intra_word_dashes = TRUE)
  corpus_data <- tm_map(corpus_data,content_transformer(removeNumbers))
  corpus_data <- tm_map(corpus_data,content_transformer(tolower))
  corpus_data <- tm_map(corpus_data,removeWords,stopwords("english"))
  corpus_data <- tm_map(corpus_data,removeWords,stopwordsXPO6)
  #corpus_data <- tm_map(corpus_data,removeWords,salutation)#we want salutation they are a new topics now
  corpus_data <- tm_map(corpus_data,content_transformer(stripWhitespace))
  dtm <- DocumentTermMatrix(corpus_data, control = list(wordLengths=c(3,30)))#from 3 to the Longest non-coined word in a major dictionary
  dtm_tfidf <- DocumentTermMatrix(corpus_data, control = list(weighting = weightTfIdf,wordLengths=c(3,30)))#tf-idf dtm# can't be used in lda
  #BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))#Require Rweka package
  BigramTokenizer <-function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)#use NLP
  dtm_bi <- DocumentTermMatrix(corpus_data, control = list(tokenize=BigramTokenizer, weighting = weightTf, wordLengths=c(3,30)))#dtm bigram tokenizer
  save(corpus_data, dtm,dtm_tfidf,dtm_bi, file="corpus_dtm_combinedRevRes.rda")
}
FILE <- "Plda_combinedRevRes.rda"
if (FILE %in% list.files()) load(FILE) else {
  omit <- slam::row_sums(dtm) != 0#I remove 0 document necessary for topicmodels
  dtm2 <- dtm[omit, ]
  goldenSetRevRes2 <- goldenSetRevRes[omit,]
  corpus_data2 <- corpus_data[omit]
  Pdelta_combined  <- matrix(0, nrow = 7 , ncol = ncol(dtm2))
  colnames(Pdelta_combined) <- colnames(dtm2)
  SeedWeight <- length(goldenSetRevRes$fullText)* 0.15 #only 15% of the total number of reviews or responses but we need to consider thqat each review have multiple sentences
  Pdelta_combined[1, c("service" , "staff" , "desk" , "reservation")]  <- SeedWeight#Service topic
  Pdelta_combined[2, c("experience" , "price" , "stay", "money")]   <- SeedWeight#Value topic
  Pdelta_combined[3, c("location" , "place", "area", "view")]  <- SeedWeight#Location topic
  Pdelta_combined[4, c("room" , "bed" , "bathroom" ,"shower")]  <- SeedWeight#Room topic, shower removed cause noot contained in it
  Pdelta_combined[5, c( "food" ,"breakfast", "bar", "restaurant" )]  <- SeedWeight#Food topic
  Pdelta_combined[6, c( "look" , "guest", "regards" ,"forward")]  <- SeedWeight#greetings and salutations topic
  Pdelta_combined[7, c( "gym" , "wi-fi", "spa" ,"pool")]  <- SeedWeight#hotel amenities
  set.seed(1000)
  Plda_combined_0.1 <- LDA(dtm2, k = 7, method = "Gibbs", seedwords = Pdelta_combined,
                        control = list(alpha = 0.1, best = TRUE, delta = 0.01 ,
                                       verbose = 0, burnin = 500, iter = 1000, thin = 100))
  Plda_combined_1 <- LDA(dtm2, k = 7, method = "Gibbs", seedwords = Pdelta_combined,
                        control = list(alpha = 1, best = TRUE, delta = 0.01 ,
                                       verbose = 0, burnin = 500, iter = 1000, thin = 100))
  Plda_combined_7 <- LDA(dtm2, k = 7, method = "Gibbs", seedwords = Pdelta_combined,
                        control = list(alpha = 7, best = TRUE, delta = 0.1 ,
                                       verbose = 0, burnin = 500, iter = 1000, thin = 100))
  save(dtm2, goldenSetRevRes2,corpus_data2, Pdelta_combined, Plda_combined_0.1, Plda_combined_1,Plda_combined_7, file = "Plda_combinedRevRes.rda")
}
FILE <- "topicsCombined.rda"
if (FILE %in% list.files()) load(FILE) else {
  theta_1 <- as.data.frame(round(Plda_combined_1@gamma, 5))
  reviewId <- Plda_combined_1@documents
  topicsCombined_1 <- data.frame(reviewId,theta_1, stringsAsFactors = FALSE)
  goldenSetRevRes_topicsCombined_1 <- merge(goldenSetRevRes2,topicsCombined_1, by= "reviewId")#taking in consideration only the subset for the evaluation
  #terms(Plda_combined_1,10)
  #terms(Plda_combined_0.1,10)
  # #topics <- topics(Plda_combined3)
  # topics0.6 <- topics(Plda_combined3, threshold=0.6)
  # topics0.7 <- topics(Plda_combined3, threshold=0.7)
  # topics0.8 <- topics(Plda_combined3, threshold=0.8)
  # topics0.9 <- topics(Plda_combined3, threshold=0.9)
  # 
  # df_topicsCombined$topics
  # breadth_review <- sapply(1:7,function(topic)tabulate(nsentences_rev[topics==topic],max(nsentences_rev)))
  # breadth_response <- sapply(1:7,function(topic)tabulate(nsentences_res[topics==topic],max(nsentences_res)))
  # breadth_review <- as.data.frame(breadth_review)
  # breadth_response <- as.data.frame(breadth_response)
  
  colnames( goldenSetRevRes_topicsCombined_1) <- c("reviewId","fullText","goup","topic1","topic2","topic3","topic4","topic5","topic6","topic7")
  # topics_1T <- apply(goldenSetRevRes_topicsCombined_1[,c(4:10)], 2,function(x) which(x>0.1))#which reviews have each topic above a certain threeshold
  #  topics_15T <- apply(goldenSetRevRes_topicsCombined_1[,c(4:10)], 2,function(x) which(x>0.15))#which reviews have each topic above a certain threeshold
  #  topics_1R <- apply(goldenSetRevRes_topicsCombined_1[,c(4:10)], 1,function(x) which(x>0.1))#which topics are in each review above the threeshold
  #goldenSetRevRes_topicsCombined_1$topics_25 <- unlist(topics_25)
  goldenSetRevRes_topicsCombined_1_rounded <- goldenSetRevRes_topicsCombined_1
  goldenSetRevRes_topicsCombined_1_rounded[,-c(1:3)] <- round(goldenSetRevRes_topicsCombined_1_rounded[,-c(1:3)],2)#if you want to round the proportions--> better for the threshold  
  

#setdiff(topics_1T[[1]], topics_15T[[1]])

theta_01 <- as.data.frame(round(Plda_combined_0.1@gamma, 5))
reviewId <- Plda_combined_0.1@documents
topicsCombined_01 <- data.frame(reviewId,theta_01, stringsAsFactors = FALSE)
goldenSetRevRes_topicsCombined_01 <- merge(goldenSetRevRes2,topicsCombined_01, by= "reviewId")
colnames( goldenSetRevRes_topicsCombined_01) <- c("reviewId","fullText","goup","topic1","topic2","topic3","topic4","topic5","topic6","topic7")
goldenSetRevRes_topicsCombined_01_rounded <- goldenSetRevRes_topicsCombined_01
goldenSetRevRes_topicsCombined_01_rounded[,-c(1:3)] <- round(goldenSetRevRes_topicsCombined_01_rounded[,-c(1:3)],2)

theta_7 <- as.data.frame(round(Plda_combined_7@gamma, 5))
reviewId <- Plda_combined_7@documents
topicsCombined_7 <- data.frame(reviewId,theta_7, stringsAsFactors = FALSE)
goldenSetRevRes_topicsCombined_7 <- merge(goldenSetRevRes2,topicsCombined_7, by= "reviewId")
colnames( goldenSetRevRes_topicsCombined_7) <- c("reviewId","fullText","goup","topic1","topic2","topic3","topic4","topic5","topic6","topic7")
goldenSetRevRes_topicsCombined_7_rounded <- goldenSetRevRes_topicsCombined_7
goldenSetRevRes_topicsCombined_7_rounded[,-c(1:3)] <- round(goldenSetRevRes_topicsCombined_7_rounded[,-c(1:3)],2)
save(goldenSetRevRes_topicsCombined_7,goldenSetRevRes_topicsCombined_1,goldenSetRevRes_topicsCombined_01, file = "topicsCombined.rda")
}
```
```{r}
kable(terms(Plda_combined_1,10),style="simple",  caption = 'Model 1 topics described by 10 most frequents words.', justify= "left")
kable(terms(Plda_combined_0.1,10),style="simple",  caption = 'Model 2 topics described by 10 most frequents words.', justify= "left")

```

\pagebreak


```{r}
kable(terms(Plda_combined_7,10),style="simple",  caption = 'Model 3 topics described by 10 most frequents words.', justify= "left")
```

\pagebreak

## Match topicmodels (TM) results with humans

In order to match the topic models results we subset the dataset of the topic model by retaining only those document on which we build our golden standard set. We did both for the full and the partial agreement. The two dataset are reported below in order. It is important to highlight that the topic models results will provide a probability for all the topics in every documents. In this way every topic will results to be in every topics. So, we rounded those probability to two digits and we then apply a threshold. In fact, while the topic models assign a probability to be in a document to all the topics, we assume that the topic is not there if the probability assigned to it by the model is below the proportion that each topic has by chance if the topics follow an uniform distribution. Given that the probability by chance of a topic to appear is 100/7 we apply this threshold of inclusion. Therefore, the topic is retained only if there is more than 14.29% probability to be there according to the algorythm. This procedure is also required because human labeling do not assign proportion to topics, it just reports when a topic is in a document and when it is not. This means some topics are absent in some documents. So, if we include all the topics as in the algorithm results even when the proportion is smaller than the probability by chance our congruence result would be biased by the model results. Moreover, we organize the topics identified by the model in decreasing proportional order. So, the topics with higher probability are reported in first position. In this case we have a limitation of our design because labelers were not suppose to establish dominance among topics because we consider that to be difficult and subjective to establish in many situations. 
```{r, warning=FALSE, results='hide'}
###alpha 1
topicM_1 <- as_data_frame(goldenSetRevRes_topicsCombined_1)[c(1,4:10)]
topicM_1_threshold <- topicM_1
topicM_1_threshold[topicM_1_threshold<0.1429] <- NA#this is I retain only topics above 14.29% probability
topicM_1$names <- apply(topicM_1[-1], 1, FUN = function(x) paste(names(head(sort(x[x!=0], decreasing = TRUE), 7)), collapse=","))# sorting topics according to their proportion in each of the review
topicM_1_threshold$names <- apply(topicM_1_threshold[-1], 1, FUN = function(x) paste(names(head(sort(x[x!=0], decreasing = TRUE), 7)), collapse=","))# sorting topics according to their proportion in each of the review
topicM_1_threshold<- topicM_1_threshold %>%
  separate(names, c("Topic1", "Topic2","Topic3","Topic4","Topic5","Topic6","Topic7"), ",")# topic reported in order of frequency
#transform the topic to their label to prepare for the comparison with the humans
topicM_1_threshold<- as_data_frame(sapply(topicM_1_threshold, function(x) replace(x, x=="topic1", "service")))
topicM_1_threshold<- as_data_frame(sapply(topicM_1_threshold, function(x) replace(x, x=="topic2", "value")))
topicM_1_threshold<- as_data_frame(sapply(topicM_1_threshold, function(x) replace(x, x=="topic3", "location")))
topicM_1_threshold<- as_data_frame(sapply(topicM_1_threshold, function(x) replace(x, x=="topic4", "room")))
topicM_1_threshold<- as_data_frame(sapply(topicM_1_threshold, function(x) replace(x, x=="topic5", "food")))
topicM_1_threshold<- as_data_frame(sapply(topicM_1_threshold, function(x) replace(x, x=="topic6", "greetings and salutations")))
topicM_1_threshold<- as_data_frame(sapply(topicM_1_threshold, function(x) replace(x, x=="topic7", "hotel amenities")))
tmvspartialAgreementgoldenset_1_threshold<- filter(topicM_1_threshold, reviewId %in% reviewTopicpartialAgreementStep5$reviewID)#taking into account only those that are in the human subset tmvsfullAgreementgoldenset_threshold<- filter(topicM_threshold, reviewId %in% agreementStep5)#taking into account only those that are in the human subset_1_threshold<- filter(topicM_threshold, reviewId %in% reviewTopicpartialAgreementStep5$reviewID)#taking into account only those that are in the human subset
tmvsfullAgreementgoldenset_1_threshold<- filter(topicM_1_threshold, reviewId %in% agreementStep5)#taking into account only those that are in the human subset
####alpha 0.1
topicM_01 <- as_data_frame(goldenSetRevRes_topicsCombined_01)[c(1,4:10)]
topicM_01_threshold <- topicM_01
topicM_01_threshold[topicM_01_threshold<0.1429] <- NA#this is I retain only topics above 14.29% probability
topicM_01$names <- apply(topicM_01[-1], 1, FUN = function(x) paste(names(head(sort(x[x!=0], decreasing = TRUE), 7)), collapse=","))# sorting topics according to their proportion in each of the review
topicM_01_threshold$names <- apply(topicM_01_threshold[-1], 1, FUN = function(x) paste(names(head(sort(x[x!=0], decreasing = TRUE), 7)), collapse=","))# sorting topics according to their proportion in each of the review
topicM_01_threshold<- topicM_01_threshold %>%
  separate(names, c("Topic1", "Topic2","Topic3","Topic4","Topic5","Topic6","Topic7"), ",")# topic reported in order of frequency
#transform the topic to their label to prepare for the comparison with the humans
topicM_01_threshold<- as_data_frame(sapply(topicM_01_threshold, function(x) replace(x, x=="topic1", "service")))
topicM_01_threshold<- as_data_frame(sapply(topicM_01_threshold, function(x) replace(x, x=="topic2", "value")))
topicM_01_threshold<- as_data_frame(sapply(topicM_01_threshold, function(x) replace(x, x=="topic3", "location")))
topicM_01_threshold<- as_data_frame(sapply(topicM_01_threshold, function(x) replace(x, x=="topic4", "room")))
topicM_01_threshold<- as_data_frame(sapply(topicM_01_threshold, function(x) replace(x, x=="topic5", "food")))
topicM_01_threshold<- as_data_frame(sapply(topicM_01_threshold, function(x) replace(x, x=="topic6", "greetings and salutations")))
topicM_01_threshold<- as_data_frame(sapply(topicM_01_threshold, function(x) replace(x, x=="topic7", "hotel amenities")))
tmvspartialAgreementgoldenset_01_threshold<- filter(topicM_01_threshold, reviewId %in% reviewTopicpartialAgreementStep5$reviewID)#taking into account only those that are in the human subset
tmvsfullAgreementgoldenset_01_threshold<- filter(topicM_01_threshold, reviewId %in% agreementStep5)#taking into account only those that are in the human subset


####alpha 7
topicM_7 <- as_data_frame(goldenSetRevRes_topicsCombined_7)[c(1,4:10)]
topicM_7_threshold <- topicM_7
topicM_7_threshold[topicM_7_threshold<0.1429] <- NA#this is I retain only topics above 14.29% probability
topicM_7$names <- apply(topicM_7[-1], 1, FUN = function(x) paste(names(head(sort(x[x!=0], decreasing = TRUE), 7)), collapse=","))# sorting topics according to their proportion in each of the review
topicM_7_threshold$names <- apply(topicM_7_threshold[-1], 1, FUN = function(x) paste(names(head(sort(x[x!=0], decreasing = TRUE), 7)), collapse=","))# sorting topics according to their proportion in each of the review
topicM_7_threshold<- topicM_7_threshold %>%
  separate(names, c("Topic1", "Topic2","Topic3","Topic4","Topic5","Topic6","Topic7"), ",")# topic reported in order of frequency
#transform the topic to their label to prepare for the comparison with the humans
topicM_7_threshold<- as_data_frame(sapply(topicM_7_threshold, function(x) replace(x, x=="topic1", "service")))
topicM_7_threshold<- as_data_frame(sapply(topicM_7_threshold, function(x) replace(x, x=="topic2", "value")))
topicM_7_threshold<- as_data_frame(sapply(topicM_7_threshold, function(x) replace(x, x=="topic3", "location")))
topicM_7_threshold<- as_data_frame(sapply(topicM_7_threshold, function(x) replace(x, x=="topic4", "room")))
topicM_7_threshold<- as_data_frame(sapply(topicM_7_threshold, function(x) replace(x, x=="topic5", "food")))
topicM_7_threshold<- as_data_frame(sapply(topicM_7_threshold, function(x) replace(x, x=="topic6", "greetings and salutations")))
topicM_7_threshold<- as_data_frame(sapply(topicM_7_threshold, function(x) replace(x, x=="topic7", "hotel amenities")))
tmvspartialAgreementgoldenset_7_threshold<- filter(topicM_7_threshold, reviewId %in% reviewTopicpartialAgreementStep5$reviewID)#taking into account only those that are in the human subset
tmvsfullAgreementgoldenset_7_threshold<- filter(topicM_7_threshold, reviewId %in% agreementStep5)#taking into account only those that are in the human subset







# topicM<- topicM %>%
#   separate(names, c("Topic1", "Topic2","Topic3","Topic4","Topic5","Topic6","Topic7"), ",")# topic reported in order of frequency
# #transform the topic to their label to prepare for the comparison with the humans
# topicM<- as_data_frame(sapply(topicM, function(x) replace(x, x=="topic1", "service")))
# topicM<- as_data_frame(sapply(topicM, function(x) replace(x, x=="topic2", "value")))
# topicM<- as_data_frame(sapply(topicM, function(x) replace(x, x=="topic3", "location")))
# topicM<- as_data_frame(sapply(topicM, function(x) replace(x, x=="topic4", "room")))
# topicM<- as_data_frame(sapply(topicM, function(x) replace(x, x=="topic5", "food")))
# topicM<- as_data_frame(sapply(topicM, function(x) replace(x, x=="topic6", "greetings and salutations")))
# topicM<- as_data_frame(sapply(topicM, function(x) replace(x, x=="topic7", "hotel amenities")))



# tmvspartialAgreementgoldenset<- filter(topicM, reviewId %in% reviewTopicpartialAgreementStep5$reviewID)#taking into account only those that are in the human subset

#tmvsfullAgreementgoldenset<- filter(topicM, reviewId %in% agreementStep5)#taking into account only those that are in the human subset

# need to create three dataset
#######1 step is to replace humans and tm topics with the number representing their topicglimpse(topicM)  
#######humans coverted in topic numbers
glimpse(reviewTopicfullAgreementStep5)
reviewTopicfullAgreementStep5$names <- apply(reviewTopicfullAgreementStep5[-c(1:2)], 1, FUN = function(x) paste(names(head(na.omit(x), 7)), collapse=","))
reviewTopicfullAgreementStep5<- reviewTopicfullAgreementStep5 %>%
  separate(names, c("Topic1H", "Topic2H","Topic3H","Topic4H","Topic5H","Topic6H","Topic7H"), ",")
glimpse(reviewTopicpartialAgreementStep5)
reviewTopicpartialAgreementStep5$names <- apply(reviewTopicpartialAgreementStep5[-c(1:2)], 1, FUN = function(x) paste(names(head(na.omit(x), 7)), collapse=","))
reviewTopicpartialAgreementStep5<- reviewTopicpartialAgreementStep5 %>%
  separate(names, c("Topic1H", "Topic2H","Topic3H","Topic4H","Topic5H","Topic6H","Topic7H"), ",")# topic identified reported topic1 doesn't mean service but the first topic reported.. necessary for comparison to the tm

########## Creating the Full agreement dataset for comparisons with topic model results
humansFull<- as_data_frame(sapply(reviewTopicfullAgreementStep5[,c(1,10:16)], function(x) replace(x, x== "service", 1)))# subsetting the dataset and replacinf the values only in the required topics
humansFull<- as_data_frame(sapply(humansFull, function(x) replace(x, x== "value", 2)))
humansFull<- as_data_frame(sapply(humansFull, function(x) replace(x, x=="location", 3)))
humansFull<- as_data_frame(sapply(humansFull, function(x) replace(x, x=="room", 4)))
humansFull<- as_data_frame(sapply(humansFull, function(x) replace(x, x== "food",5)))
humansFull<- as_data_frame(sapply(humansFull, function(x) replace(x, x=="greetings and salutations", 6)))
humansFull<- as_data_frame(sapply(humansFull, function(x) replace(x, x== "hotel amenities", 7)))
humansFull[is.na(humansFull)] <- 0 #replace NA with 0 


########### Creating the partial agreement dataset for comparisons with topic model results

humansPartial<- as_data_frame(sapply(reviewTopicpartialAgreementStep5[,c(1,10:16)], function(x) replace(x, x== "service", 1)))# subsetting the dataset and replacinf the values only in the required topics
humansPartial<- as_data_frame(sapply(humansPartial, function(x) replace(x, x== "value", 2)))
humansPartial<- as_data_frame(sapply(humansPartial, function(x) replace(x, x=="location", 3)))
humansPartial<- as_data_frame(sapply(humansPartial, function(x) replace(x, x=="room", 4)))
humansPartial<- as_data_frame(sapply(humansPartial, function(x) replace(x, x== "food",5)))
humansPartial<- as_data_frame(sapply(humansPartial, function(x) replace(x, x=="greetings and salutations", 6)))
humansPartial<- as_data_frame(sapply(humansPartial, function(x) replace(x, x== "hotel amenities", 7)))
humansPartial[is.na(humansPartial)] <- 0 #replace NA with 0 
###################convert topic in position, rememebr proportion are not ordered like the topics.. find a way to order them too --using full dataset##################
# tmodel_full <- as_data_frame(sapply(tmvsfullAgreementgoldenset, function(x) replace(x, x== "service", 1)))
# tmodel_full<- as_data_frame(sapply(tmodel_full, function(x) replace(x, x== "value", 2)))
# tmodel_full<- as_data_frame(sapply(tmodel_full, function(x) replace(x, x=="location", 3)))
# tmodel_full<- as_data_frame(sapply(tmodel_full, function(x) replace(x, x=="room", 4)))
# tmodel_full<- as_data_frame(sapply(tmodel_full, function(x) replace(x, x== "food",5)))
# tmodel_full<- as_data_frame(sapply(tmodel_full, function(x) replace(x, x=="greetings and salutations", 6)))
# tmodel_full<- as_data_frame(sapply(tmodel_full, function(x) replace(x, x== "hotel amenities", 7)))


######alpha 1
tmodel_full_1_threshold <- as_data_frame(sapply(tmvsfullAgreementgoldenset_1_threshold, function(x) replace(x, x== "service", 1)))
tmodel_full_1_threshold<- as_data_frame(sapply(tmodel_full_1_threshold, function(x) replace(x, x== "value", 2)))
tmodel_full_1_threshold<- as_data_frame(sapply(tmodel_full_1_threshold, function(x) replace(x, x=="location", 3)))
tmodel_full_1_threshold<- as_data_frame(sapply(tmodel_full_1_threshold, function(x) replace(x, x=="room", 4)))
tmodel_full_1_threshold<- as_data_frame(sapply(tmodel_full_1_threshold, function(x) replace(x, x== "food",5)))
tmodel_full_1_threshold<- as_data_frame(sapply(tmodel_full_1_threshold, function(x) replace(x, x=="greetings and salutations", 6)))
tmodel_full_1_threshold<- as_data_frame(sapply(tmodel_full_1_threshold, function(x) replace(x, x== "hotel amenities", 7)))

tmodel_full_1_threshold[is.na(tmodel_full_1_threshold)] <- 0 #replace NA with 0 


###############convert topic in position, rememebr proportion are not ordered like the topics.. find a way to order them too --using partial dataset################
# tmodel_partial <- as_data_frame(sapply(tmvspartialAgreementgoldenset, function(x) replace(x, x== "service", 1)))
# tmodel_partial<- as_data_frame(sapply(tmodel_partial, function(x) replace(x, x== "value", 2)))
# tmodel_partial<- as_data_frame(sapply(tmodel_partial, function(x) replace(x, x=="location", 3)))
# tmodel_partial<- as_data_frame(sapply(tmodel_partial, function(x) replace(x, x=="room", 4)))
# tmodel_partial<- as_data_frame(sapply(tmodel_partial, function(x) replace(x, x== "food",5)))
# tmodel_partial<- as_data_frame(sapply(tmodel_partial, function(x) replace(x, x=="greetings and salutations", 6)))
# tmodel_partial<- as_data_frame(sapply(tmodel_partial, function(x) replace(x, x== "hotel amenities", 7)))

tmodel_partial_1_threshold <- as_data_frame(sapply(tmvspartialAgreementgoldenset_1_threshold, function(x) replace(x, x== "service", 1)))
tmodel_partial_1_threshold<- as_data_frame(sapply(tmodel_partial_1_threshold, function(x) replace(x, x== "value", 2)))
tmodel_partial_1_threshold<- as_data_frame(sapply(tmodel_partial_1_threshold, function(x) replace(x, x=="location", 3)))
tmodel_partial_1_threshold<- as_data_frame(sapply(tmodel_partial_1_threshold, function(x) replace(x, x=="room", 4)))
tmodel_partial_1_threshold<- as_data_frame(sapply(tmodel_partial_1_threshold, function(x) replace(x, x== "food",5)))
tmodel_partial_1_threshold<- as_data_frame(sapply(tmodel_partial_1_threshold, function(x) replace(x, x=="greetings and salutations", 6)))
tmodel_partial_1_threshold<- as_data_frame(sapply(tmodel_partial_1_threshold, function(x) replace(x, x== "hotel amenities", 7)))

tmodel_partial_1_threshold[is.na(tmodel_partial_1_threshold)] <- 0 #replace NA with 0 

############ alpha 01
tmodel_full_01_threshold <- as_data_frame(sapply(tmvsfullAgreementgoldenset_01_threshold, function(x) replace(x, x== "service", 1)))
tmodel_full_01_threshold<- as_data_frame(sapply(tmodel_full_01_threshold, function(x) replace(x, x== "value", 2)))
tmodel_full_01_threshold<- as_data_frame(sapply(tmodel_full_01_threshold, function(x) replace(x, x=="location", 3)))
tmodel_full_01_threshold<- as_data_frame(sapply(tmodel_full_01_threshold, function(x) replace(x, x=="room", 4)))
tmodel_full_01_threshold<- as_data_frame(sapply(tmodel_full_01_threshold, function(x) replace(x, x== "food",5)))
tmodel_full_01_threshold<- as_data_frame(sapply(tmodel_full_01_threshold, function(x) replace(x, x=="greetings and salutations", 6)))
tmodel_full_01_threshold<- as_data_frame(sapply(tmodel_full_01_threshold, function(x) replace(x, x== "hotel amenities", 7)))
tmodel_full_01_threshold[is.na(tmodel_full_01_threshold)] <- 0 #replace NA with 0 
tmodel_partial_01_threshold <- as_data_frame(sapply(tmvspartialAgreementgoldenset_01_threshold, function(x) replace(x, x== "service", 1)))
tmodel_partial_01_threshold<- as_data_frame(sapply(tmodel_partial_01_threshold, function(x) replace(x, x== "value", 2)))
tmodel_partial_01_threshold<- as_data_frame(sapply(tmodel_partial_01_threshold, function(x) replace(x, x=="location", 3)))
tmodel_partial_01_threshold<- as_data_frame(sapply(tmodel_partial_01_threshold, function(x) replace(x, x=="room", 4)))
tmodel_partial_01_threshold<- as_data_frame(sapply(tmodel_partial_01_threshold, function(x) replace(x, x== "food",5)))
tmodel_partial_01_threshold<- as_data_frame(sapply(tmodel_partial_01_threshold, function(x) replace(x, x=="greetings and salutations", 6)))
tmodel_partial_01_threshold<- as_data_frame(sapply(tmodel_partial_01_threshold, function(x) replace(x, x== "hotel amenities", 7)))
tmodel_partial_01_threshold[is.na(tmodel_partial_01_threshold)] <- 0 #replace NA with 0 



############ alpha 7
tmodel_full_7_threshold <- as_data_frame(sapply(tmvsfullAgreementgoldenset_7_threshold, function(x) replace(x, x== "service", 1)))
tmodel_full_7_threshold<- as_data_frame(sapply(tmodel_full_7_threshold, function(x) replace(x, x== "value", 2)))
tmodel_full_7_threshold<- as_data_frame(sapply(tmodel_full_7_threshold, function(x) replace(x, x=="location", 3)))
tmodel_full_7_threshold<- as_data_frame(sapply(tmodel_full_7_threshold, function(x) replace(x, x=="room", 4)))
tmodel_full_7_threshold<- as_data_frame(sapply(tmodel_full_7_threshold, function(x) replace(x, x== "food",5)))
tmodel_full_7_threshold<- as_data_frame(sapply(tmodel_full_7_threshold, function(x) replace(x, x=="greetings and salutations", 6)))
tmodel_full_7_threshold<- as_data_frame(sapply(tmodel_full_7_threshold, function(x) replace(x, x== "hotel amenities", 7)))
tmodel_full_7_threshold[is.na(tmodel_full_7_threshold)] <- 0 #replace NA with 0 
tmodel_partial_7_threshold <- as_data_frame(sapply(tmvspartialAgreementgoldenset_7_threshold, function(x) replace(x, x== "service", 1)))
tmodel_partial_7_threshold<- as_data_frame(sapply(tmodel_partial_7_threshold, function(x) replace(x, x== "value", 2)))
tmodel_partial_7_threshold<- as_data_frame(sapply(tmodel_partial_7_threshold, function(x) replace(x, x=="location", 3)))
tmodel_partial_7_threshold<- as_data_frame(sapply(tmodel_partial_7_threshold, function(x) replace(x, x=="room", 4)))
tmodel_partial_7_threshold<- as_data_frame(sapply(tmodel_partial_7_threshold, function(x) replace(x, x== "food",5)))
tmodel_partial_7_threshold<- as_data_frame(sapply(tmodel_partial_7_threshold, function(x) replace(x, x=="greetings and salutations", 6)))
tmodel_partial_7_threshold<- as_data_frame(sapply(tmodel_partial_7_threshold, function(x) replace(x, x== "hotel amenities", 7)))
tmodel_partial_7_threshold[is.na(tmodel_partial_7_threshold)] <- 0 #replace NA with 0 




#2 step report the proportion of those topics in the tm case
# df_full <- bind_cols(humansFull,tmodel_full[,9:15])
# df_full_threshold <- bind_cols(humansFull,tmodel_full_threshold[,9:15])#consider topic modle topics only if their proportion is above 0.05
# df_partial <- bind_cols(humansPartial,tmodel_partial[,9:15])
# df_partial_threshold <- bind_cols(humansPartial,tmodel_partial_threshold[,9:15])#consider topic modle topics only if their proportion is above 0.05


```

 

```{r, warning= FALSE, results='hide'}
## Congruence among humans

#Before moving to the model selection we report the congrunce between review and response as the topics are idenitfied in the golden standard set.

library(lsa)
library(textmineR)
humansFull$topics7 <- paste( humansFull$Topic1H,humansFull$Topic2H,humansFull$Topic3H,humansFull$Topic4H,humansFull$Topic5H,humansFull$Topic6H,humansFull$Topic7H, sep=",")
#humansFull$Topics5 <- paste( humansFull$Topic1,humansFull$Topic2,humansFull$Topic3,humansFull$Topic4,humansFull$Topic5, sep=",")
#rv <- arrange(rv, id)#reorder the review by id
rv <- humansFull[grep("^RV",humansFull$reviewID),]#dataframe review
rv$id <- as.integer(gsub("RV", "",rv$reviewID))
colnames(rv) <-c("reviewIdRev" ,"topic1rev","topic2rev","topic3rev","topic4rev","topic5rev","topic6rev","topic7rev", "topicsRev7",  "id")
rs <- humansFull[grep("^RS", humansFull$reviewID),]#dataframe response
rs$id <- as.integer(gsub("RS", "",rs$reviewID))
colnames(rs) <-c("reviewIdRes"  ,"topic1res","topic2res","topic3res","topic4res","topic5res","topic6res","topic7res", "topicsRes7", "id")
# rv5$id <- as.integer(gsub("RS", "",rv5$reviewId))
# colnames(rv5) <- c("reviewID","topics_RV","id")
# rs5$id <- as.integer(gsub("RS", "",rs5$reviewId))
# #rs <- arrange(rs, id)#reorder the response by id
# colnames(rs5) <- c("reviewID","topics_RS","id")#if I merge on id no need to sort them
# #table(rv$id%in%rs$id)
dfHuman <- merge(rv,rs, by= "id")#dataset of pairs with full agreement between humans
cosineTopicsCongruence7 <- function(x) {
    trv <- as.numeric(unlist(strsplit(x[[10]], split=",")))
    trs <- as.numeric(unlist(strsplit(x[[19]], split=",")))
    # topics <- c(1,2,3,4,5)
    # brv <- ifelse(topics%in%trv,1,0)#breadth vector --> sort(unique(trs))#  I don't need to sort and unique them
    # brs <- ifelse(topics%in%trs,1,0)
    congruenceCos <- round(cosine(trv,trs),5)
  }
dfHuman$cosineBreadthCongruence7 <- apply(dfHuman,1, cosineTopicsCongruence7)# cosine congruence for pair human labeled, important to notice that the absence of a topic (0) is used in the calculation. I think it make sense because it means the response and the review don't discuss the same topics. But inflate the concruence among them 
#ggplot(dfHuman, aes(x=cosineBreadthCongruence7)) + geom_density(fill="springgreen1")+ 
#labs(x="Figure 1: Cosine Topics Congruence") +coord_cartesian(ylim = c(0, 6)) #####plotting congruence

```


##Select the model close to humans-  Assessment of interpretability of topic models


The topics are converted in numeric vectors and cosine similarity and hellinger distance are used to assess how close are the different topic models' results to the golden standard set. We will report first the results of the three models with the full agreement golden standard and later those of partial agreement. 



##### Full agreement 
```{r}

######alpha 1
#humanFull I have already added the vector of the topics for the humans above but required
tmodel_full_1_threshold$topics7TM <- paste( tmodel_full_1_threshold$Topic1,tmodel_full_1_threshold$Topic2,tmodel_full_1_threshold$Topic3,tmodel_full_1_threshold$Topic4,tmodel_full_1_threshold$Topic5,tmodel_full_1_threshold$Topic6,tmodel_full_1_threshold$Topic7, sep=",")
tmodel_full_1_threshold$reviewID <- tmodel_full_1_threshold$reviewId#add one variable at the end with the same name of humanFull for the marge--> improve by changing the name of the variable before
dfmodel_1 <- merge(humansFull[,c(1,9)],tmodel_full_1_threshold[,c(16:17)], by= "reviewID")#dataset that contains only ID and vectors of topics
  cosineTopicsCongruence7 <- function(x) {
    trH <- as.numeric(unlist(strsplit(x[[2]], split=",")))
    trTM <- as.numeric(unlist(strsplit(x[[3]], split=",")))
    # topics <- c(1,2,3,4,5)
    # brv <- ifelse(topics%in%trv,1,0)#breadth vector --> sort(unique(trs))#  I don't need to sort and unique them
    # brs <- ifelse(topics%in%trs,1,0)
    congruenceCos <- round(cosine(trH,trTM),5)
  }
  dfmodel_1$cosineCongruence7 <- apply(dfmodel_1,1, cosineTopicsCongruence7)# for each document in the dataset I know how close is the model to the golden set labelad by human.. for each model run I will have a df and the model selcted will be the one with the highest average similarity with the golden set (human)
 
p1_cosine <- ggplot(dfmodel_1, aes(x=cosineCongruence7)) + geom_density(fill="red")+ 
labs(x="Cosine Congruence Model 1") +coord_cartesian(ylim = c(0, 6)) #####plotting congruence


hellingerCong7 <- function(x) {
    trH <- as.numeric(unlist(strsplit(x[[2]], split=",")))
    trTM <- as.numeric(unlist(strsplit(x[[3]], split=",")))
    congruenceHellinger <- round(CalcHellingerDist(trH,trTM),5)
  }
  dfmodel_1$HellingerCong7 <- apply(dfmodel_1,1, hellingerCong7) 
p1_hellinger<- ggplot(dfmodel_1, aes(x=HellingerCong7)) + geom_density(fill="springgreen1")+ 
labs(x="Hellinger Congruence Model 1") +coord_cartesian(ylim = c(0, 6)) 

#########alpha 0.1
#humanFull I have already added the vector of the topics for the humans above but required
tmodel_full_01_threshold$topics7TM <- paste( tmodel_full_01_threshold$Topic1,tmodel_full_01_threshold$Topic2,tmodel_full_01_threshold$Topic3,tmodel_full_01_threshold$Topic4,tmodel_full_01_threshold$Topic5,tmodel_full_01_threshold$Topic6,tmodel_full_01_threshold$Topic7, sep=",")
tmodel_full_01_threshold$reviewID <- tmodel_full_01_threshold$reviewId#add one variable at the end with the same name of humanFull for the marge--> improve by changing the name of the variable before
dfmodel_01 <- merge(humansFull[,c(1,9)],tmodel_full_01_threshold[,c(16:17)], by= "reviewID")#dataset that contains only ID and vectors of topics
  cosineTopicsCongruence7 <- function(x) {
    trH <- as.numeric(unlist(strsplit(x[[2]], split=",")))
    trTM <- as.numeric(unlist(strsplit(x[[3]], split=",")))
    # topics <- c(1,2,3,4,5)
    # brv <- ifelse(topics%in%trv,1,0)#breadth vector --> sort(unique(trs))#  I don't need to sort and unique them
    # brs <- ifelse(topics%in%trs,1,0)
    congruenceCos <- round(cosine(trH,trTM),5)
  }
  dfmodel_01$cosineCongruence7 <- apply(dfmodel_01,1, cosineTopicsCongruence7)# for each document in the dataset I know how close is the model to the golden set labelad by human.. for each model run I will have a df and the model selcted will be the one with the highest average similarity with the golden set (human)
 
p01_cosine<- ggplot(dfmodel_01, aes(x=cosineCongruence7)) + geom_density(fill="red")+ 
labs(x="Cosine Congruence Model 2") +coord_cartesian(ylim = c(0, 6)) #####plotting congruence


hellingerCong7 <- function(x) {
    trH <- as.numeric(unlist(strsplit(x[[2]], split=",")))
    trTM <- as.numeric(unlist(strsplit(x[[3]], split=",")))
    congruenceHellinger <- round(CalcHellingerDist(trH,trTM),5)
  }
  dfmodel_01$HellingerCong7 <- apply(dfmodel_01,1, hellingerCong7) 
p01_hellinger <- ggplot(dfmodel_01, aes(x=HellingerCong7)) + geom_density(fill="springgreen1")+ 
labs(x="Hellinger Congruence Model 2") +coord_cartesian(ylim = c(0, 6)) 

#########alpha 7
#humanFull I have already added the vector of the topics for the humans above but required
tmodel_full_7_threshold$topics7TM <- paste(tmodel_full_7_threshold$Topic1,tmodel_full_7_threshold$Topic2,tmodel_full_7_threshold$Topic3,tmodel_full_7_threshold$Topic4,tmodel_full_7_threshold$Topic5,tmodel_full_7_threshold$Topic6,tmodel_full_7_threshold$Topic7, sep=",")
tmodel_full_7_threshold$reviewID <- tmodel_full_7_threshold$reviewId#add one variable at the end with the same name of humanFull for the marge--> improve by changing the name of the variable before
dfmodel_7 <- merge(humansFull[,c(1,9)],tmodel_full_7_threshold[,c(16:17)], by= "reviewID")#dataset that contains only ID and vectors of topics
  cosineTopicsCongruence7 <- function(x) {
    trH <- as.numeric(unlist(strsplit(x[[2]], split=",")))
    trTM <- as.numeric(unlist(strsplit(x[[3]], split=",")))
    # topics <- c(1,2,3,4,5)
    # brv <- ifelse(topics%in%trv,1,0)#breadth vector --> sort(unique(trs))#  I don't need to sort and unique them
    # brs <- ifelse(topics%in%trs,1,0)
    congruenceCos <- round(cosine(trH,trTM),5)
  }
  dfmodel_7$cosineCongruence7 <- apply(dfmodel_7,1, cosineTopicsCongruence7)# for each document in the dataset I know how close is the model to the golden set labelad by human.. for each model run I will have a df and the model selcted will be the one with the highest average similarity with the golden set (human)
 
p7cosine <- ggplot(dfmodel_7, aes(x=cosineCongruence7)) + geom_density(fill="red")+ 
labs(x="Cosine Congruence Model 3") +coord_cartesian(ylim = c(0, 6)) #####plotting congruence


hellingerCong7 <- function(x) {
    trH <- as.numeric(unlist(strsplit(x[[2]], split=",")))
    trTM <- as.numeric(unlist(strsplit(x[[3]], split=",")))
    congruenceHellinger <- round(CalcHellingerDist(trH,trTM),5)
  }
  dfmodel_7$HellingerCong7 <- apply(dfmodel_7,1, hellingerCong7) 
p7hellinger<- ggplot(dfmodel_7, aes(x=HellingerCong7)) + geom_density(fill="springgreen1")+ 
labs(x="Hellinger Congruence Model 3") +coord_cartesian(ylim = c(0, 6)) 



```

\begin{center}
Table 7 Descriptives: Full agreement
\end{center}

Descriptive | Model 1  | Model 2  | Model 3 
------ | ----------- |--------------- | ------------- 
Cosine min  | `r round(min(dfmodel_1$cosineCongruence7),2)`   | `r round(min(dfmodel_01$cosineCongruence7),2)` | `r round(min(dfmodel_7$cosineCongruence7),2)` 
Hellinger min |  `r round(min(dfmodel_1$HellingerCong7),2)`   | `r round(min(dfmodel_01$HellingerCong7),2)` | `r round(min(dfmodel_7$HellingerCong7),2)`
Cosine mean  | `r round(mean(dfmodel_1$cosineCongruence7), 2)`  | `r round(mean(dfmodel_01$cosineCongruence7), 2)` | `r round(mean(dfmodel_7$cosineCongruence7), 2)` 
Hellinger mean |  `r round(mean(dfmodel_1$HellingerCong7), 2)`  | `r round(mean(dfmodel_01$HellingerCong7), 2)` | `r round(mean(dfmodel_7$HellingerCong7), 2)` 
Cosine median | `r round(median(dfmodel_1$cosineCongruence7),2)`   | `r round(median(dfmodel_01$cosineCongruence7),2)` | `r round(median(dfmodel_7$cosineCongruence7),2)` 
Hellinger median |  `r round(median(dfmodel_1$HellingerCong7),2)`   | `r round(median(dfmodel_01$HellingerCong7),2)` | `r round(median(dfmodel_7$HellingerCong7),2)` 
Cosine max  | `r round(max(dfmodel_1$cosineCongruence7),2)` | `r round(max(dfmodel_01$cosineCongruence7),2)` | `r round(max(dfmodel_7$cosineCongruence7),2)`
Hellinger max |  `r round(max(dfmodel_1$HellingerCong7),2)` | `r round(max(dfmodel_01$HellingerCong7),2)` | `r round(max(dfmodel_7$HellingerCong7),2)`
Cosine sd  | `r round(sd(dfmodel_1$cosineCongruence7),2)` |  `r round(sd(dfmodel_01$cosineCongruence7),2)` | `r round(sd(dfmodel_7$cosineCongruence7),2)` 
Hellinger sd | `r round(sd(dfmodel_1$HellingerCong7),2)` |  `r round(sd(dfmodel_01$HellingerCong7),2)` | `r round(sd(dfmodel_7$HellingerCong7),2)`


By looking at the results in Table 7 it is clear that Model 2 is outperforming the other two models. In fact, in interpreting the results we need to take in mind that for the cosine similarity higher values are better while for the hellinger distance lower values are better.



```{r}
library(gridExtra)
grid.arrange(p1_cosine, p1_hellinger, p01_cosine, p01_hellinger, p7cosine,p7hellinger, ncol=2, top=" Figure 3: Models vs golden standard set")
```



##### Partial agreement 

We plan to perform the same steps for the full agreement also for the partial partial agreement dataset. However, before to complete this section and move to this last step (the creation of the congruence measure) we would like to have feedback on how to improve the current report. It seems that the idea to present the new method using 3 different models achieve the scope of this report. We are able to select a model based on its interpretability evaluated by humans.  

```{r}
humansPartial$topics7 <- paste( humansPartial$Topic1H,humansPartial$Topic2H,humansPartial$Topic3H,humansPartial$Topic4H,humansPartial$Topic5H,humansPartial$Topic6H,humansPartial$Topic7H, sep=",")
#humansPartial$Topics5 <- paste( humansPartial$Topic1,humansPartial$Topic2,humansPartial$Topic3,humansPartial$Topic4,humansPartial$Topic5, sep=",")
#rv <- arrange(rv, id)#reorder the review by id
rv_partial <- humansPartial[grep("^RV",humansPartial$reviewID),]#dataframe review
rv_partial$id <- as.integer(gsub("RV", "",rv_partial$reviewID))
colnames(rv_partial) <-c("reviewIdRev" ,"topic1rev","topic2rev","topic3rev","topic4rev","topic5rev","topic6rev","topic7rev", "topicsRev7",  "id")
rs_partial <- humansPartial[grep("^RS", humansPartial$reviewID),]#dataframe response
rs_partial$id <- as.integer(gsub("RS", "",rs_partial$reviewID))
colnames(rs_partial) <-c("reviewIdRes"  ,"topic1res","topic2res","topic3res","topic4res","topic5res","topic6res","topic7res", "topicsRes7", "id")
# rv5$id <- as.integer(gsub("RS", "",rv5$reviewId))
# colnames(rv5) <- c("reviewID","topics_RV","id")
# rs5$id <- as.integer(gsub("RS", "",rs5$reviewId))
# #rs <- arrange(rs, id)#reorder the response by id
# colnames(rs5) <- c("reviewID","topics_RS","id")#if I merge on id no need to sort them
# #table(rv$id%in%rs$id)
dfHuman_partial <- merge(rv_partial,rs_partial, by= "id")#dataset of pairs with Partial agreement between humans
cosineTopicsCongruence7 <- function(x) {
    trv_partial <- as.numeric(unlist(strsplit(x[[10]], split=",")))
    trs_partial <- as.numeric(unlist(strsplit(x[[19]], split=",")))
    # topics <- c(1,2,3,4,5)
    # brv <- ifelse(topics%in%trv,1,0)#breadth vector --> sort(unique(trs))#  I don't need to sort and unique them
    # brs <- ifelse(topics%in%trs,1,0)
    congruenceCos <- round(cosine(trv_partial,trs_partial),5)
  }
dfHuman_partial$cosineBreadthCongruence7 <- apply(dfHuman_partial,1, cosineTopicsCongruence7)
######alpha 1
#humanPartial I have already added the vector of the topics for the humans above but required
tmodel_partial_1_threshold$topics7TM <- paste( tmodel_partial_1_threshold$Topic1,tmodel_partial_1_threshold$Topic2,tmodel_partial_1_threshold$Topic3,tmodel_partial_1_threshold$Topic4,tmodel_partial_1_threshold$Topic5,tmodel_partial_1_threshold$Topic6,tmodel_partial_1_threshold$Topic7, sep=",")
tmodel_partial_1_threshold$reviewID <- tmodel_partial_1_threshold$reviewId#add one variable at the end with the same name of humanpartial for the marge--> improve by changing the name of the variable before
dfmodel_partial_1 <- merge(humansPartial[,c(1,9)],tmodel_partial_1_threshold[,c(16:17)], by= "reviewID")#dataset that contains only ID and vectors of topics
  cosineTopicsCongruence7 <- function(x) {
    trH <- as.numeric(unlist(strsplit(x[[2]], split=",")))
    trTM <- as.numeric(unlist(strsplit(x[[3]], split=",")))
    # topics <- c(1,2,3,4,5)
    # brv <- ifelse(topics%in%trv,1,0)#breadth vector --> sort(unique(trs))#  I don't need to sort and unique them
    # brs <- ifelse(topics%in%trs,1,0)
    congruenceCos <- round(cosine(trH,trTM),5)
  }
  dfmodel_partial_1$cosineCongruence7 <- apply(dfmodel_partial_1,1, cosineTopicsCongruence7)# for each document in the dataset I know how close is the model to the golden set labelad by human.. for each model run I will have a df and the model selcted will be the one with the highest average similarity with the golden set (human)
 
p1_cosine_partial <- ggplot(dfmodel_partial_1, aes(x=cosineCongruence7)) + geom_density(fill="red")+ 
labs(x="Cosine Congruence Model 1") +coord_cartesian(ylim = c(0, 6)) #####plotting congruence


hellingerCong7 <- function(x) {
    trH <- as.numeric(unlist(strsplit(x[[2]], split=",")))
    trTM <- as.numeric(unlist(strsplit(x[[3]], split=",")))
    congruenceHellinger <- round(CalcHellingerDist(trH,trTM),5)
  }
  dfmodel_partial_1$HellingerCong7 <- apply(dfmodel_partial_1,1, hellingerCong7) 
p1_hellinger_partial<- ggplot(dfmodel_partial_1, aes(x=HellingerCong7)) + geom_density(fill="springgreen1")+ 
labs(x="Hellinger Congruence Model 1") +coord_cartesian(ylim = c(0, 6)) 

#########alpha 0.1
#humanpartial I have already added the vector of the topics for the humans above but required
tmodel_partial_01_threshold$topics7TM <- paste( tmodel_partial_01_threshold$Topic1,tmodel_partial_01_threshold$Topic2,tmodel_partial_01_threshold$Topic3,tmodel_partial_01_threshold$Topic4,tmodel_partial_01_threshold$Topic5,tmodel_partial_01_threshold$Topic6,tmodel_partial_01_threshold$Topic7, sep=",")
tmodel_partial_01_threshold$reviewID <- tmodel_partial_01_threshold$reviewId#add one variable at the end with the same name of humanpartial for the marge--> improve by changing the name of the variable before
dfmodel_partial_01 <- merge(humansPartial[,c(1,9)],tmodel_partial_01_threshold[,c(16:17)], by= "reviewID")#dataset that contains only ID and vectors of topics
  cosineTopicsCongruence7 <- function(x) {
    trH <- as.numeric(unlist(strsplit(x[[2]], split=",")))
    trTM <- as.numeric(unlist(strsplit(x[[3]], split=",")))
    # topics <- c(1,2,3,4,5)
    # brv <- ifelse(topics%in%trv,1,0)#breadth vector --> sort(unique(trs))#  I don't need to sort and unique them
    # brs <- ifelse(topics%in%trs,1,0)
    congruenceCos <- round(cosine(trH,trTM),5)
  }
  dfmodel_partial_01$cosineCongruence7 <- apply(dfmodel_partial_01,1, cosineTopicsCongruence7)# for each document in the dataset I know how close is the model to the golden set labelad by human.. for each model run I will have a df and the model selcted will be the one with the highest average similarity with the golden set (human)
 
p01_cosine_partial <- ggplot(dfmodel_partial_01, aes(x=cosineCongruence7)) + geom_density(fill="red")+ 
labs(x="Cosine Congruence Model 1") +coord_cartesian(ylim = c(0, 6)) #####plotting congruence


hellingerCong7 <- function(x) {
    trH <- as.numeric(unlist(strsplit(x[[2]], split=",")))
    trTM <- as.numeric(unlist(strsplit(x[[3]], split=",")))
    congruenceHellinger <- round(CalcHellingerDist(trH,trTM),5)
  }
  dfmodel_partial_01$HellingerCong7 <- apply(dfmodel_partial_01,1, hellingerCong7) 
p01_hellinger_partial<- ggplot(dfmodel_partial_01, aes(x=HellingerCong7)) + geom_density(fill="springgreen1")+ 
labs(x="Hellinger Congruence Model 1") +coord_cartesian(ylim = c(0, 6)) 


#########alpha 7
#humanpartial I have already added the vector of the topics for the humans above but required
tmodel_partial_7_threshold$topics7TM <- paste( tmodel_partial_7_threshold$Topic1,tmodel_partial_7_threshold$Topic2,tmodel_partial_7_threshold$Topic3,tmodel_partial_7_threshold$Topic4,tmodel_partial_7_threshold$Topic5,tmodel_partial_7_threshold$Topic6,tmodel_partial_7_threshold$Topic7, sep=",")
tmodel_partial_7_threshold$reviewID <- tmodel_partial_7_threshold$reviewId#add one variable at the end with the same name of humanpartial for the marge--> improve by changing the name of the variable before
dfmodel_partial_7 <- merge(humansPartial[,c(1,9)],tmodel_partial_7_threshold[,c(16:17)], by= "reviewID")#dataset that contains only ID and vectors of topics
  cosineTopicsCongruence7 <- function(x) {
    trH <- as.numeric(unlist(strsplit(x[[2]], split=",")))
    trTM <- as.numeric(unlist(strsplit(x[[3]], split=",")))
    # topics <- c(1,2,3,4,5)
    # brv <- ifelse(topics%in%trv,1,0)#breadth vector --> sort(unique(trs))#  I don't need to sort and unique them
    # brs <- ifelse(topics%in%trs,1,0)
    congruenceCos <- round(cosine(trH,trTM),5)
  }
  dfmodel_partial_7$cosineCongruence7 <- apply(dfmodel_partial_7,1, cosineTopicsCongruence7)# for each document in the dataset I know how close is the model to the golden set labelad by human.. for each model run I will have a df and the model selcted will be the one with the highest average similarity with the golden set (human)
 
p7_cosine_partial <- ggplot(dfmodel_partial_7, aes(x=cosineCongruence7)) + geom_density(fill="red")+ 
labs(x="Cosine Congruence Model 1") +coord_cartesian(ylim = c(0, 6)) #####plotting congruence


hellingerCong7 <- function(x) {
    trH <- as.numeric(unlist(strsplit(x[[2]], split=",")))
    trTM <- as.numeric(unlist(strsplit(x[[3]], split=",")))
    congruenceHellinger <- round(CalcHellingerDist(trH,trTM),5)
  }
  dfmodel_partial_7$HellingerCong7 <- apply(dfmodel_partial_7,1, hellingerCong7) 
p7_hellinger_partial<- ggplot(dfmodel_partial_7, aes(x=HellingerCong7)) + geom_density(fill="springgreen1")+ 
labs(x="Hellinger Congruence Model 1") +coord_cartesian(ylim = c(0, 6)) 


```
\begin{center}
Table 8 Descriptives: Partial agreement
\end{center}

Descriptive | Model 1  | Model 2  | Model 3 
------ | ----------- |--------------- | ------------- 
Cosine min  | `r round(min(dfmodel_partial_1$cosineCongruence7),2)`   | `r round(min(dfmodel_partial_01$cosineCongruence7),2)` | `r round(min(dfmodel_partial_7$cosineCongruence7),2)` 
Hellinger min |  `r round(min(dfmodel_partial_1$HellingerCong7),2)`   | `r round(min(dfmodel_partial_01$HellingerCong7),2)` | `r round(min(dfmodel_partial_7$HellingerCong7),2)`
Cosine mean  | `r round(mean(dfmodel_partial_1$cosineCongruence7), 2)`  | `r round(mean(dfmodel_partial_01$cosineCongruence7), 2)` | `r round(mean(dfmodel_partial_7$cosineCongruence7), 2)` 
Hellinger mean |  `r round(mean(dfmodel_partial_1$HellingerCong7), 2)`  | `r round(mean(dfmodel_partial_01$HellingerCong7), 2)` | `r round(mean(dfmodel_partial_7$HellingerCong7), 2)` 
Cosine median | `r round(median(dfmodel_partial_1$cosineCongruence7),2)`   | `r round(median(dfmodel_partial_01$cosineCongruence7),2)` | `r round(median(dfmodel_partial_7$cosineCongruence7),2)` 
Hellinger median |  `r round(median(dfmodel_partial_1$HellingerCong7),2)`   | `r round(median(dfmodel_partial_01$HellingerCong7),2)` | `r round(median(dfmodel_partial_7$HellingerCong7),2)` 
Cosine max  | `r round(max(dfmodel_partial_1$cosineCongruence7),2)` | `r round(max(dfmodel_partial_01$cosineCongruence7),2)` | `r round(max(dfmodel_partial_7$cosineCongruence7),2)`
Hellinger max |  `r round(max(dfmodel_partial_1$HellingerCong7),2)` | `r round(max(dfmodel_partial_01$HellingerCong7),2)` | `r round(max(dfmodel_partial_7$HellingerCong7),2)`
Cosine sd  | `r round(sd(dfmodel_partial_1$cosineCongruence7),2)` |  `r round(sd(dfmodel_partial_01$cosineCongruence7),2)` | `r round(sd(dfmodel_partial_7$cosineCongruence7),2)` 
Hellinger sd | `r round(sd(dfmodel_partial_1$HellingerCong7),2)` |  `r round(sd(dfmodel_partial_01$HellingerCong7),2)` | `r round(sd(dfmodel_partial_7$HellingerCong7),2)`

```{r}
library(gridExtra)
grid.arrange(p1_cosine_partial, p1_hellinger_partial, p01_cosine_partial, p01_hellinger_partial, p7_cosine_partial,p7_hellinger_partial, ncol=2, top=" Figure 4: Models vs golden standard set Partial")
```

 